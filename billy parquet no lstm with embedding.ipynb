{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.3\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_parquet('E:/train_low_mem.parquet')\n",
    "df.drop(['row_id'], inplace=True, axis=1)\n",
    "investment_id = df.pop(\"investment_id\")\n",
    "y = df.pop(\"target\")\n",
    "\n",
    "investment_ids = list(investment_id.unique())\n",
    "investment_id_lookup_layer = IntegerLookup(oov_token=-1, output_mode='int')\n",
    "investment_id_lookup_layer.adapt(np.array(investment_ids))\n",
    "vocab = investment_id_lookup_layer.get_vocabulary(include_special_tokens=True)\n",
    "vocab_size = investment_id_lookup_layer.vocabulary_size()\n",
    "print(max(investment_ids))\n",
    "print(vocab_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    # TODO\n",
    "    return x, y\n",
    "\n",
    "def make_dataset(feature, investment_id, y, batch_size=2048, mode='train'):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature), y))\n",
    "    ds = ds.map(preprocess)\n",
    "    if mode == \"train\": ds = ds.shuffle(8192, seed=69)  # only shuffle when training\n",
    "    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "# Pearson correlation coefficient loss and metrics\n",
    "def pearson_corr(y_true, y_pred, axis=-1):\n",
    "    return tfp.stats.correlation(tf.squeeze(y_pred), tf.squeeze(y_true), sample_axis=axis, event_axis=None)\n",
    "\n",
    "def pearson_corr_loss(y_true, y_pred, axis=-1):\n",
    "    return (1 - pearson_corr(y_true, y_pred, axis=axis)) + 2 * tf.keras.metrics.mean_squared_error(y_true, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Model\n",
    "def build_model():\n",
    "    investment_id_inputs = Input((1, ), name='inv_id_in', dtype=tf.uint16)\n",
    "    features_inputs = Input((300, ), name='feature_in', dtype=tf.float16)\n",
    "\n",
    "    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n",
    "    investment_id_x = Embedding(vocab_size, 64, input_length=1)(investment_id_x)\n",
    "    investment_id_x = Reshape((-1, ))(investment_id_x)\n",
    "    investment_id_x = Dense(64, activation='swish')(investment_id_x)\n",
    "    # investment_id_x = Dropout(0.2)(investment_id_x)\n",
    "    investment_id_x = Dense(64, activation='swish')(investment_id_x)\n",
    "    # investment_id_x = Dropout(0.2)(investment_id_x)\n",
    "    investment_id_x = Dense(64, activation='swish')(investment_id_x)\n",
    "    # investment_id_x = Dropout(0.2)(investment_id_x)\n",
    "\n",
    "    feature_x = GaussianNoise(0.01)(features_inputs)\n",
    "    feature_x = Dense(256, activation='swish')(feature_x)\n",
    "    # feature_x = Dropout(0.2)(feature_x)\n",
    "    feature_x = Dense(256, activation='swish')(feature_x)\n",
    "    # feature_x = Dropout(0.2)(feature_x)\n",
    "    feature_x = Dense(256, activation='swish')(feature_x)\n",
    "    # feature_x = Dropout(0.2)(feature_x)\n",
    "\n",
    "    x = Concatenate(axis=1)([investment_id_x, feature_x])\n",
    "    x = Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    # x = LayerNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    # x = LayerNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    # x = LayerNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    xOut = Dense(1, name='output')(x)\n",
    "\n",
    "    model = Model(inputs=[investment_id_inputs, features_inputs], outputs=[xOut])\n",
    "    model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=7e-4), metrics=[pearson_corr, 'mse'])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.summary()\n",
    "plot_model(model, to_file='model.png', show_shapes=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "\n",
    "class GroupTimeSeriesSplit:\n",
    "    \"\"\"\n",
    "    Custom class to create a Group Time Series Split. We ensure\n",
    "    that the time id values that are in the testing data are not a part\n",
    "    of the training data & the splits are temporal\n",
    "    \"\"\"\n",
    "    def __init__(self, n_folds: int, holdout_size: int, groups: str) -> None:\n",
    "        self.n_folds = n_folds\n",
    "        self.holdout_size = holdout_size\n",
    "        self.groups = groups\n",
    "\n",
    "    def split(self, X) -> Tuple[np.array, np.array]:\n",
    "        # Take the group column and get the unique values\n",
    "        unique_time_ids = np.unique(self.groups.values)\n",
    "\n",
    "        # Split the time ids into the length of the holdout size\n",
    "        # and reverse so we work backwards in time. Also, makes\n",
    "        # it easier to get the correct time_id values per\n",
    "        # split\n",
    "        array_split_time_ids = np.array_split(\n",
    "            unique_time_ids, len(unique_time_ids) // self.holdout_size\n",
    "        )[::-1]\n",
    "\n",
    "        # Get the first n_folds values\n",
    "        array_split_time_ids = array_split_time_ids[:self.n_folds]\n",
    "\n",
    "        for time_ids in array_split_time_ids:\n",
    "            # Get test index - time id values that are in the time_ids\n",
    "            test_condition = X['time_id'].isin(time_ids)\n",
    "            test_index = X.loc[test_condition].index\n",
    "\n",
    "            # Get train index - The train index will be the time\n",
    "            # id values right up until the minimum value in the test\n",
    "            # data - we can also add a gap to this step by\n",
    "            # time id < (min - gap)\n",
    "            train_condition = X['time_id'] < (np.min(time_ids))\n",
    "            train_index = X.loc[train_condition].index\n",
    "\n",
    "            yield train_index, test_index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import scipy\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(n_splits=6, shuffle=True, random_state=69)\n",
    "# group_time_series_split = GroupTimeSeriesSplit(n_folds=6, holdout_size=200, groups=df['time_id'])\n",
    "models = []\n",
    "\n",
    "for index, (train_indices, valid_indices) in enumerate(kfold.split(df, investment_id)):\n",
    "    print(f'Model {index}')\n",
    "    X_train, X_val = df.iloc[train_indices].loc[:, df.columns != 'time_id'], df.iloc[valid_indices].loc[:, df.columns != 'time_id']\n",
    "    investment_id_train = investment_id[train_indices]\n",
    "    y_train, y_val = y.iloc[train_indices], y.iloc[valid_indices]\n",
    "    investment_id_val = investment_id[valid_indices]\n",
    "    train_ds = make_dataset(X_train, investment_id_train, y_train)\n",
    "    valid_ds = make_dataset(X_val, investment_id_val, y_val, mode=\"valid\")\n",
    "    model = build_model()\n",
    "    callbacks = [tf.keras.callbacks.ModelCheckpoint(f\"model_{index}\", monitor='val_mse', save_best_only=True, mode='min'),\n",
    "                 tf.keras.callbacks.EarlyStopping(patience=13, monitor='val_mse', verbose=1, restore_best_weights=True, mode='min'),\n",
    "                 tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=6, verbose=1, monitor='val_mse', mode='min'),\n",
    "                 tf.keras.callbacks.TerminateOnNaN()]\n",
    "    history = model.fit(train_ds, epochs=100, validation_data=valid_ds, callbacks=callbacks)\n",
    "    models.append(load_model(f\"model_{index}\", custom_objects={'pearson_corr': pearson_corr, 'pearson_corr_loss': pearson_corr_loss}))\n",
    "\n",
    "    pearson_score = scipy.stats.pearsonr(model.predict(valid_ds).ravel(), y_val.values)[0]\n",
    "    print('Pearson:', pearson_score)\n",
    "    del investment_id_train\n",
    "    del investment_id_val\n",
    "    del X_train\n",
    "    del X_val\n",
    "    del y_train\n",
    "    del y_val\n",
    "    del train_ds\n",
    "    del valid_ds\n",
    "    del model\n",
    "    gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1228/1228 [==============================] - 11s 7ms/step - loss: 0.9932 - pearson_corr: 0.1395 - mse: 0.8369 - val_loss: 0.8095 - val_pearson_corr: 0.1212 - val_mse: 0.8062\n",
      "Epoch 2/100\n",
      "1228/1228 [==============================] - 8s 7ms/step - loss: 0.8351 - pearson_corr: 0.1595 - mse: 0.8319 - val_loss: 0.8114 - val_pearson_corr: 0.1109 - val_mse: 0.8082\n",
      "Epoch 3/100\n",
      "1228/1228 [==============================] - 7s 5ms/step - loss: 0.8325 - pearson_corr: 0.1711 - mse: 0.8288 - val_loss: 0.8133 - val_pearson_corr: 0.1189 - val_mse: 0.8097\n",
      "Epoch 4/100\n",
      "1228/1228 [==============================] - 6s 5ms/step - loss: 0.8291 - pearson_corr: 0.1825 - mse: 0.8254 - val_loss: 0.8097 - val_pearson_corr: 0.1304 - val_mse: 0.8064\n",
      "Epoch 5/100\n",
      "1228/1228 [==============================] - 6s 5ms/step - loss: 0.8261 - pearson_corr: 0.1930 - mse: 0.8219 - val_loss: 0.8198 - val_pearson_corr: 0.1173 - val_mse: 0.8151\n",
      "Epoch 6/100\n",
      "1228/1228 [==============================] - 6s 5ms/step - loss: 0.8226 - pearson_corr: 0.2034 - mse: 0.8184 - val_loss: 0.8378 - val_pearson_corr: 0.1078 - val_mse: 0.8321\n",
      "Epoch 7/100\n",
      "1228/1228 [==============================] - 6s 5ms/step - loss: 0.8199 - pearson_corr: 0.2133 - mse: 0.8150 - val_loss: 0.8214 - val_pearson_corr: 0.1051 - val_mse: 0.8162\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 8/100\n",
      "1228/1228 [==============================] - 6s 5ms/step - loss: 0.8075 - pearson_corr: 0.2407 - mse: 0.8036 - val_loss: 0.8288 - val_pearson_corr: 0.1089 - val_mse: 0.8248\n",
      "Epoch 9/100\n",
      "1228/1228 [==============================] - 6s 5ms/step - loss: 0.8050 - pearson_corr: 0.2468 - mse: 0.8010 - val_loss: 0.8275 - val_pearson_corr: 0.1098 - val_mse: 0.8236\n",
      "Epoch 10/100\n",
      "1228/1228 [==============================] - 6s 5ms/step - loss: 0.8038 - pearson_corr: 0.2500 - mse: 0.7997 - val_loss: 0.8287 - val_pearson_corr: 0.1096 - val_mse: 0.8247\n",
      "Epoch 11/100\n",
      "1228/1228 [==============================] - 6s 5ms/step - loss: 0.8026 - pearson_corr: 0.2529 - mse: 0.7984 - val_loss: 0.8305 - val_pearson_corr: 0.1048 - val_mse: 0.8265\n",
      "Epoch 12/100\n",
      "1228/1228 [==============================] - 6s 5ms/step - loss: 0.8016 - pearson_corr: 0.2552 - mse: 0.7974 - val_loss: 0.8365 - val_pearson_corr: 0.0998 - val_mse: 0.8323\n",
      "Epoch 13/100\n",
      "1228/1228 [==============================] - 6s 5ms/step - loss: 0.8006 - pearson_corr: 0.2577 - mse: 0.7963 - val_loss: 0.8333 - val_pearson_corr: 0.1059 - val_mse: 0.8291\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 14/100\n",
      "1228/1228 [==============================] - 6s 5ms/step - loss: 0.7986 - pearson_corr: 0.2617 - mse: 0.7944 - val_loss: 0.8350 - val_pearson_corr: 0.1076 - val_mse: 0.8308\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "X_train, y_train = df.loc[:, df.columns != 'time_id'], y\n",
    "investment_id_train = investment_id\n",
    "# train_ds = make_dataset(X_train, investment_id_train, y_train)\n",
    "model = build_model()\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(f\"model_comb\", monitor='val_mse', save_best_only=True, mode='min'),\n",
    "             tf.keras.callbacks.EarlyStopping(patience=13, monitor='val_mse', verbose=1, restore_best_weights=True, mode='min'),\n",
    "             tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=6, verbose=1, monitor='val_mse', mode='min'),\n",
    "             tf.keras.callbacks.TerminateOnNaN()]\n",
    "history = model.fit({'inv_id_in': investment_id_train, 'feature_in': X_train}, y_train, batch_size=2048, epochs=100, validation_split=0.2, callbacks=callbacks)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set. ;)\n",
      "       row_id    target\n",
      "18211     8_1  0.018524\n",
      "18212     8_2  0.075989\n",
      "18213     8_6  0.122681\n",
      "18214     8_7 -0.062744\n",
      "18215     8_8  0.002537\n",
      "...       ...       ...\n",
      "19995  8_2961 -0.119019\n",
      "19996  8_2963  0.004261\n",
      "19997  8_2964 -0.448730\n",
      "19998  8_2965 -0.013519\n",
      "19999  8_2966 -0.126831\n",
      "\n",
      "[1789 rows x 2 columns]\n",
      "Pearson: (       row_id    target  prediction\n",
      "0         0_1 -0.300875    0.103760\n",
      "1         0_2 -0.231040   -0.076843\n",
      "2         0_6  0.568807    0.052124\n",
      "3         0_7 -1.064780   -0.157593\n",
      "4         0_8 -0.531940    0.002863\n",
      "...       ...       ...         ...\n",
      "19995  8_2961  2.395875   -0.119019\n",
      "19996  8_2963  0.293830    0.004261\n",
      "19997  8_2964  0.295464   -0.448730\n",
      "19998  8_2965 -0.722136   -0.013519\n",
      "19999  8_2966 -0.268326   -0.126831\n",
      "\n",
      "[20000 rows x 3 columns], 0.20382864275050025)\n"
     ]
    }
   ],
   "source": [
    "from ubiquantEmulator import TimeSeriesAPI\n",
    "\n",
    "def preprocess_test(investment_id, feature):\n",
    "    return (investment_id, feature), 0\n",
    "\n",
    "def make_test_dataset(feature, investment_id, batch_size=2048):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n",
    "    ds = ds.map(preprocess_test)\n",
    "    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def inference(models, ds):\n",
    "    y_preds = []\n",
    "    for model in models:\n",
    "        y_pred = model.predict(ds)\n",
    "        y_preds.append(y_pred)\n",
    "    return np.mean(y_preds, axis=0)  # ensemble\n",
    "\n",
    "models = [load_model(f\"model_{index}\", custom_objects={'pearson_corr': pearson_corr, 'pearson_corr_loss': pearson_corr_loss}) for index in range(6)]\n",
    "models.pop(3)\n",
    "# models = [load_model(f\"model_comb\", custom_objects={'pearson_corr': pearson_corr, 'pearson_corr_loss': pearson_corr_loss})]\n",
    "# models += [load_model(f\"1/model_{index}\", custom_objects={'pearson_corr': pearson_corr, 'pearson_corr_loss': pearson_corr_loss}) for index in range(6)]\n",
    "# test = pd.read_parquet('E:/train_low_mem.parquet')\n",
    "# time_ids = df['time_id'].unique()\n",
    "# test = test.loc[test['time_id'].isin(time_ids)]  # take last 200 time id rows from df\n",
    "test = pd.read_csv('kaggle/input/ubiquant-market-prediction/train.csv', engine='pyarrow')\n",
    "iter_test = TimeSeriesAPI(test)\n",
    "for n, (test_df, sample_prediction_df) in enumerate(iter_test):\n",
    "    ds = make_test_dataset(test_df.loc[:, 'f_0':'f_299'], test_df[\"investment_id\"])\n",
    "    sample_prediction_df['target'] = inference(models, ds)\n",
    "    iter_test.predict(sample_prediction_df)\n",
    "print(sample_prediction_df)\n",
    "print(f'Pearson: {iter_test.score()}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ubiquant'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[1;32mIn [9]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mubiquant\u001B[39;00m\n\u001B[0;32m      2\u001B[0m env \u001B[38;5;241m=\u001B[39m ubiquant\u001B[38;5;241m.\u001B[39mmake_env()\n\u001B[0;32m      3\u001B[0m iter_test \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39miter_test()\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'ubiquant'"
     ]
    }
   ],
   "source": [
    "import ubiquant\n",
    "env = ubiquant.make_env()\n",
    "iter_test = env.iter_test()\n",
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "    ds = make_test_dataset(test_df.loc[:, 'f_0':'f_299'], test_df[\"investment_id\"])\n",
    "    sample_prediction_df['target'] = inference(models, ds)\n",
    "    env.predict(sample_prediction_df)\n",
    "print(sample_prediction_df)\n",
    "print(f'Pearson: {scipy.stats.pearsonr(sample_prediction_df[\"target\"].values, test_df[\"target\"].values)[0]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}