DNN x5 v1: 0.147 with embedding and evenly spaced dropouts, 0.0973287412239429 on test, 0.15045034440418906 on full set
DNN x5 v2: 0.141 with embedding and (accidental) continuous dropouts 0.13677389718485403 on test 0.17883023572228965 on full set
DNN x5 v3: 0.151 with embedding and no dropouts, reduced model size, 0.15255 on test 0.18895457032593518 on full set
DNN x5 v4: 0.117 no embedding poor performance so no model weights saved
DNN x5 v5: -0.122 pearson loss with callback monitored on val_loss, 40 epoch -0.09318488819089013 on test
DNN x6 v6: 0.151 but better than V3 mse with 50 epoch, 6-fold split 0.16307406390342696 on test, 0.22849779460548547 on holdout time id test (but not accurate because the model is trained on them already), 0.19251263798788032 on full set
DNN x7 v7: 0.149 50 epoch, 0.02 gaussian, 7-fold split 0.16838214767224044 on test, 0.20536024110463943 on full set
DNN x6 v8: 0.143 fixed pearson loss with callback monitored on val_pearson_corr (except reduceLR is monitored on val_loss), 50 epoch, 6-fold split, 0.10661272847612188 on test, 0.15126692629921581 on full set
ensemble of dnn x6 v8, x5 v3 and x6 v6 gave 0.144
DNN x6 v9: 0.136 fixed pearson loss with all callback monitored on val_pearson_corr, 100 epoch, 6-fold split, reduced model params, 0.2351341413109163 on test
DNN x6 v10: 0.136 pearson loss + sigmoid(mse) * 2, 100 epoch, 6-fold split, reduced model, params 0.2520307069205498 on test
DNN x6 v11: 0.143 mse, 100 epoch, 6-fold split, 900K model 0.18482995603167732 on test
DNN x5 v12: 0.109 mse, 100 epoch, 6-fold group time series split but only first 5 trained model (6th have error), 600k model (same as 0.151) 0.1753688485062298 on test csv, 0.12107241807225483 on holdout time id test, if combine with model_5 of DNN x6 v11, 0.146224652668455 on holdout test
DNN x5 v13: 0.139 mse, 100 epoch, 6-fold time id k fold using sklearn not group split, same model as 0.151 but initial lr=1e-4, 0.19157831178997678 on csv test, 0.2343073816777293 on holdout test, 0.2306751129366612 on full set
DNN x1 v14: 0.113 mse, 100 epoch, single fold trained on all data, smaller model, overfitting, lr=1e-3, 0.1519516690172568 on full set (to submit)
DNN x6 v15: 0.141 (submit all 6), 0.136 (without model 3), mse, 100 epoch, 6 fold by investment id, same model as 0.151, lr=7e-4, model 3 is bad 0.20017738381748112 on test csv
DNN x6 V16: 0.150, reduced model size, lr=8e-4, 6 fold investment id split
DNN x6 v17: 0.141 same model as 0.151, lr=1e-4, 6 fold investment id split, conditional hybrid loss (mse>0.83 -> mse, else pearson_corr) 0.17631688734188294 on test csv (to submit)
DNN x6 v18: 0.141 same model as 0.151, lr=1e-4, 6 fold investment id split, mse, batchsize=8192, 0.16533647263857085 on test csv
DNN x6 v19: 0.137 same model as 0.151, lr=1e-4, 6 fold investment id split, pearson mixed loss (non conditional), batchsize=8192, 0.18417589353211655 on test csv
DNN x6 v20: 0.145 no embedding, lr=1e-3, modified model, 6 fold investment id split, mse, batchsize=10240, ~0.21 on test csv
DNN x6 v21: 0.146, same model as 0.145 but more dropout, rest same, 0.1725336102913356 on test csv
ensemble of v20 and v21: 0.147
DNN x6 v22: 0.147 same model as v21 but replaced swish to mish and optimizer is ranger(RAdam), 50 epoch 0.18155053250323044 on test csv
ensemble of v20, 21, 22: 0.148
DNN x6 v23: 0.141 with embedding, drop out, layer norm, 50 epoch replaced swish to mish and optimizer is ranger(RAdam), 0.18041558535599808 on test csv
DNN x6 v24: 0.142 100 epoch no embedding same model as v22, 6 fold time id split, 0.20424749101033426 on test csv
hybrid inference of embedding: v3, 6, 16 no embedding: v20, 21, 22:
DNN x6 V25: 0.148 (better than ensemble of 20, 21, 22) no embedding, replaced first dense 512 with 2 dense 256, rest same as v22, 0.16504570750966377 on test csv
DNN x6 v26: 0.145 with model 3, 0.146 without model 3, increased dropout, gussian=0.1, rest same as v25 (exclude model 3, very bad) (to submit)
DNN x6 v27: 0.143, 100 epoch, gussian=0.1, rest same as v25, 0.17103911086843124 on test csv (to submit)
DNN x6 V28: 0.134, 50 epoch, extra dense256 and dropout sets, no activity reg, reduced dropout, lr=1e-2, scaled down target, 0.16+ on test csv
DNN x6 v29: 0.140 50 epoch, extra dense256 and dropout sets, no activity reg, reduced dropout, lr=1e-2, 0.18142325503702794 on test csv
DNN x5 v30: 0.134, kaggle notebook dnn model, kaggle pearson loss, 40 epoch, 1e-3, no reducelr, 0.03 on test csv (0.18 on 620k)
ensemble of v3, 6, 16, 30:
DNN x5 v31: kaggle notebook dnn model, kaggle pearson loss, 40 epoch, 1e-3, with reduce lr, 0.13 on test csv
to try v29 but lr=1e-3 restored dropout of v25

ensemble of v20, 21, 22, 25, 26 give 0.149 best for no embedding so far
hybrid of v3, 6, 16, 20, 21, 22, 25, 26 give 0.152, a bit worse than without v26
to try: reg at front dense layers

remove outliner -> model do not learn

ensemble 2 0.151 (v3, v6) on kaggle: 0.152
ensemble 3 (v3, v6, v7) on kaggle: 0.152 too but worse than ensemble 2 0.151 as lb didnt move
ensemble 3 (v3, v6, v16) on kaggle: 0.152 too but higher than previous ensemble of 2
